{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d16f297",
   "metadata": {},
   "source": [
    "# 20211015 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17f5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Embedding, Convolution1D,Dropout,Bidirectional,Merge,Reshape,GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization, Conv1D, LeakyReLU, Flatten,Masking,concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv1D,Conv2D, BatchNormalization,GlobalMaxPooling1D, GlobalAveragePooling1D,Permute, Dropout\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D,multiply, concatenate, Activation, Masking\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import keras.utils as np_utils\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b24b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile=np.load('../data/feature_cn_500.npz',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e32759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSF_feature = npzfile['HSF'] \n",
    "label = HSF_feature[:,0]\n",
    "HSF = HSF_feature[:,1:1583]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bdcdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# label = npzfile['label'] \n",
    "labels = LabelBinarizer().fit_transform(label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc989e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531, 500, 76)\n"
     ]
    }
   ],
   "source": [
    "LLD = npzfile['LLD'] \n",
    "#HDF = npzfile['HSF'] \n",
    "print(LLD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cb55ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5531, 1, 1581)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HSF = HSF.reshape(HSF.shape[0],1,HSF.shape[1])\n",
    "HSF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5376ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4423, 500, 76)\n",
      "(4423, 1, 1581)\n",
      "(4423, 794)\n"
     ]
    }
   ],
   "source": [
    "LLD_train = LLD[:4423,:]\n",
    "LLD_test = LLD[4423:,:]\n",
    "HSF_train = HSF[:4423,:]\n",
    "HSF_test = HSF[4423:,:]\n",
    "labels_train = labels[:4423]\n",
    "labels_test = labels[4423:]\n",
    "print(LLD_train.shape)\n",
    "print(HSF_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158153d5",
   "metadata": {},
   "source": [
    "# LLD模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b4b8f",
   "metadata": {},
   "source": [
    "## 注意力模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder attention function for model3\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None,\n",
    "                           timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "    # Arguments\n",
    "    x: input tensor.\n",
    "    w: weight matrix.\n",
    "    b: optional bias vector.\n",
    "    dropout: wether to apply dropout (same dropout mask\n",
    "        for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "        # Returns\n",
    "            Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.shape(w)[1]\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "    \n",
    "class AttentionDecoder(Recurrent):\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bad042",
   "metadata": {},
   "source": [
    "## 注意力模型2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                            input_dim=None, output_dim=None,\n",
    "                            timesteps=None, training=None):\n",
    "    \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        w: weight matrix.\n",
    "        b: optional bias vector.\n",
    "        dropout: wether to apply dropout (same dropout mask\n",
    "            for every temporal slice of the input).\n",
    "        input_dim: integer; optional dimensionality of the input.\n",
    "        output_dim: integer; optional dimensionality of the output.\n",
    "        timesteps: integer; optional number of timesteps.\n",
    "        training: training phase tensor or boolean.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor.\n",
    "    \"\"\"\n",
    "    if not input_dim:\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        output_dim = K.int_shape(w)[1]\n",
    "\n",
    "    if dropout is not None and 0. < dropout < 1.:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "    x = K.dot(x, w)\n",
    "    if b is not None:\n",
    "        x = K.bias_add(x, b)\n",
    "    # reshape to 3D tensor\n",
    "    if K.backend() == 'tensorflow':\n",
    "        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
    "        x.set_shape([None, None, output_dim])\n",
    "    else:\n",
    "        x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x\n",
    "\n",
    "\n",
    "class AttentionLSTM(Recurrent):\n",
    "    \"\"\"Long-Short Term Memory unit - with Attention.\n",
    "\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](keras/activations.md)).\n",
    "            If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step\n",
    "            (see [activations](keras/activations.md)).\n",
    "        attention_activation: Activation function to use\n",
    "            for the attention step. If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "            (see [activations](keras/activations.md)).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix,\n",
    "            used for the linear transformation of the recurrent state.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        attention_initializer: Initializer for the `attention_kernel` weights\n",
    "            matrix, used for the linear transformation of the inputs.\n",
    "            (see [initializers](../initializers.md)).\n",
    "        unit_forget_bias: Boolean.\n",
    "            If True, add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        recurrent_regularizer: Regularizer function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        attention_regularizer: Regularizer function applied to\n",
    "            the `attention_kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        recurrent_constraint: Constraint function applied to\n",
    "            the `recurrent_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "        attention_constraint: Constraint function applied to\n",
    "            the `attention_kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the inputs.\n",
    "        recurrent_dropout: Float between 0 and 1.\n",
    "            Fraction of the units to drop for\n",
    "            the linear transformation of the recurrent state.\n",
    "        return_attention: Returns the attention vector instead of\n",
    "            the internal state.\n",
    "\n",
    "    # References\n",
    "        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n",
    "        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
    "        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
    "        - [Bahdanau, Cho & Bengio (2014), \"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "        - [Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel & Bengio (2016), \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\"](http://arxiv.org/pdf/1502.03044.pdf)\n",
    "    \"\"\"\n",
    "    #@interfaces.legacy_recurrent_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 attention_activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 attention_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 unit_forget_bias=True,\n",
    "                 kernel_regularizer=None,\n",
    "                 recurrent_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 attention_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 attention_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 return_attention=False,\n",
    "                 implementation=1,\n",
    "                 **kwargs):\n",
    "        super(AttentionLSTM, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.attention_activation = activations.get(attention_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.attention_initializer = initializers.get(attention_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.attention_regularizer = regularizers.get(attention_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.attention_constraint = constraints.get(attention_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.return_attention = return_attention\n",
    "        self.state_spec = [InputSpec(shape=(None, self.units)),\n",
    "                           InputSpec(shape=(None, self.units))]\n",
    "        self.implementation = implementation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        batch_size = input_shape[0] if self.stateful else None\n",
    "        self.timestep_dim = input_shape[1]\n",
    "        self.input_dim = input_shape[2]\n",
    "        self.input_spec[0] = InputSpec(shape=(batch_size, None, self.input_dim))\n",
    "\n",
    "        self.states = [None, None]\n",
    "        if self.stateful:\n",
    "            self.reset_states()\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(self.input_dim, self.units * 4),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        # add attention kernel\n",
    "        self.attention_kernel = self.add_weight(\n",
    "            shape=(self.input_dim, self.units * 4),\n",
    "            name='attention_kernel',\n",
    "            initializer=self.attention_initializer,\n",
    "            regularizer=self.attention_regularizer,\n",
    "            constraint=self.attention_constraint)\n",
    "\n",
    "        # add attention weights\n",
    "        # weights for attention model\n",
    "        self.attention_weights = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                                 name='attention_W',\n",
    "                                                 initializer=self.attention_initializer,\n",
    "                                                 regularizer=self.attention_regularizer,\n",
    "                                                 constraint=self.attention_constraint)\n",
    "\n",
    "        self.attention_recurrent_weights = self.add_weight(shape=(self.units, self.units),\n",
    "                                                           name='attention_U',\n",
    "                                                           initializer=self.recurrent_initializer,\n",
    "                                                           regularizer=self.recurrent_regularizer,\n",
    "                                                           constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "                def bias_initializer(shape, *args, **kwargs):\n",
    "                    return K.concatenate([\n",
    "                        self.bias_initializer((self.units,), *args, **kwargs),\n",
    "                        initializers.Ones()((self.units,), *args, **kwargs),\n",
    "                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n",
    "                    ])\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "\n",
    "            self.attention_bias = self.add_weight(shape=(self.units,),\n",
    "                                                  name='attention_b',\n",
    "                                                  initializer=self.bias_initializer,\n",
    "                                                  regularizer=self.bias_regularizer,\n",
    "                                                  constraint=self.bias_constraint)\n",
    "\n",
    "            self.attention_recurrent_bias = self.add_weight(shape=(self.units, 1),\n",
    "                                                            name='attention_v',\n",
    "                                                            initializer=self.bias_initializer,\n",
    "                                                            regularizer=self.bias_regularizer,\n",
    "                                                            constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.attention_bias = None\n",
    "            self.attention_recurrent_bias = None\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.units]\n",
    "        self.kernel_f = self.kernel[:, self.units: self.units * 2]\n",
    "        self.kernel_c = self.kernel[:, self.units * 2: self.units * 3]\n",
    "        self.kernel_o = self.kernel[:, self.units * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n",
    "\n",
    "        self.attention_i = self.attention_kernel[:, :self.units]\n",
    "        self.attention_f = self.attention_kernel[:, self.units: self.units * 2]\n",
    "        self.attention_c = self.attention_kernel[:, self.units * 2: self.units * 3]\n",
    "        self.attention_o = self.attention_kernel[:, self.units * 3:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_i = self.bias[:self.units]\n",
    "            self.bias_f = self.bias[self.units: self.units * 2]\n",
    "            self.bias_c = self.bias[self.units * 2: self.units * 3]\n",
    "            self.bias_o = self.bias[self.units * 3:]\n",
    "        else:\n",
    "            self.bias_i = None\n",
    "            self.bias_f = None\n",
    "            self.bias_c = None\n",
    "            self.bias_o = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def preprocess_input(self, inputs, training=None):\n",
    "        return inputs\n",
    "\n",
    "    def get_constants(self, inputs, training=None):\n",
    "        constants = []\n",
    "        if self.implementation != 0 and 0 < self.dropout < 1:\n",
    "            input_shape = K.int_shape(inputs)\n",
    "            input_dim = input_shape[-1]\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, int(input_dim)))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                        ones,\n",
    "                                        training=training) for _ in range(4)]\n",
    "            constants.append(dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "        if 0 < self.recurrent_dropout < 1:\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.recurrent_dropout)\n",
    "            rec_dp_mask = [K.in_train_phase(dropped_inputs,\n",
    "                                            ones,\n",
    "                                            training=training) for _ in range(4)]\n",
    "            constants.append(rec_dp_mask)\n",
    "        else:\n",
    "            constants.append([K.cast_to_floatx(1.) for _ in range(4)])\n",
    "\n",
    "        # append the input as well for use later\n",
    "        constants.append(inputs)\n",
    "        return constants\n",
    "\n",
    "    def step(self, inputs, states):\n",
    "        h_tm1 = states[0]\n",
    "        c_tm1 = states[1]\n",
    "        dp_mask = states[2]\n",
    "        rec_dp_mask = states[3]\n",
    "        x_input = states[4]\n",
    "\n",
    "        # alignment model\n",
    "        h_att = K.repeat(h_tm1, self.timestep_dim)\n",
    "        att = _time_distributed_dense(x_input, self.attention_weights, self.attention_bias,\n",
    "                                      output_dim=K.int_shape(self.attention_weights)[1])\n",
    "        attention_ = self.attention_activation(K.dot(h_att, self.attention_recurrent_weights) + att)\n",
    "        attention_ = K.squeeze(K.dot(attention_, self.attention_recurrent_bias), 2)\n",
    "\n",
    "        alpha = K.exp(attention_)\n",
    "\n",
    "        if dp_mask is not None:\n",
    "            alpha *= dp_mask[0]\n",
    "\n",
    "        alpha /= K.sum(alpha, axis=1, keepdims=True)\n",
    "        alpha_r = K.repeat(alpha, self.input_dim)\n",
    "        alpha_r = K.permute_dimensions(alpha_r, (0, 2, 1))\n",
    "\n",
    "        # make context vector (soft attention after Bahdanau et al.)\n",
    "        z_hat = x_input * alpha_r\n",
    "        context_sequence = z_hat\n",
    "        z_hat = K.sum(z_hat, axis=1)\n",
    "\n",
    "        if self.implementation == 2:\n",
    "            z = K.dot(inputs * dp_mask[0], self.kernel)\n",
    "            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)\n",
    "            z += K.dot(z_hat, self.attention_kernel)\n",
    "\n",
    "            if self.use_bias:\n",
    "                z = K.bias_add(z, self.bias)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            i = self.recurrent_activation(z0)\n",
    "            f = self.recurrent_activation(z1)\n",
    "            c = f * c_tm1 + i * self.activation(z2)\n",
    "            o = self.recurrent_activation(z3)\n",
    "        else:\n",
    "            if self.implementation == 0:\n",
    "                x_i = inputs[:, :self.units]\n",
    "                x_f = inputs[:, self.units: 2 * self.units]\n",
    "                x_c = inputs[:, 2 * self.units: 3 * self.units]\n",
    "                x_o = inputs[:, 3 * self.units:]\n",
    "            elif self.implementation == 1:\n",
    "                x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i\n",
    "                x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f\n",
    "                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c\n",
    "                x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o\n",
    "            else:\n",
    "                raise ValueError('Unknown `implementation` mode.')\n",
    "\n",
    "            i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel_i)\n",
    "                                              + K.dot(z_hat, self.attention_i))\n",
    "            f = self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1], self.recurrent_kernel_f)\n",
    "                                          + K.dot(z_hat, self.attention_f))\n",
    "            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2], self.recurrent_kernel_c)\n",
    "                                                + K.dot(z_hat, self.attention_c))\n",
    "            o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3], self.recurrent_kernel_o)\n",
    "                                          + K.dot(z_hat, self.attention_o))\n",
    "        h = o * self.activation(c)\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            h._uses_learning_phase = True\n",
    "\n",
    "        if self.return_attention:\n",
    "            return context_sequence, [h, c]\n",
    "        else:\n",
    "            return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'activation': activations.serialize(self.activation),\n",
    "                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n",
    "                  'attention_activation': activations.serialize(self.attention_activation),\n",
    "                  'use_bias': self.use_bias,\n",
    "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n",
    "                  'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "                  'attention_initializer': initializers.serialize(self.attention_initializer),\n",
    "                  'unit_forget_bias': self.unit_forget_bias,\n",
    "                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n",
    "                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                  'attention_regularizer': regularizers.serialize(self.attention_regularizer),\n",
    "                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n",
    "                  'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "                  'attention_constraint': constraints.serialize(self.attention_constraint),\n",
    "                  'dropout': self.dropout,\n",
    "                  'recurrent_dropout': self.recurrent_dropout,\n",
    "                  'return_attention': self.return_attention}\n",
    "        base_config = super(AttentionLSTM, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d15f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1acaeb35",
   "metadata": {},
   "source": [
    "## MALSTM_FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转置\n",
    "LLD_train_FCN=LLD_train.transpose(0,2,1)\n",
    "LLD_test_FCN=LLD_test.transpose(0,2,1)\n",
    "LLD_train_FCN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALSTM_FCN():\n",
    "    ip = Input(shape=(76, 500))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    #x = AttentionLSTM(8)(x)\n",
    "    x = AttentionLSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10342456",
   "metadata": {},
   "outputs": [],
   "source": [
    "MALSTM_FCN= MALSTM_FCN()\n",
    "MALSTM_FCN.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d830b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_FCN = MALSTM_FCN.fit(LLD_train_FCN, labels_train, batch_size=64, epochs=100, verbose=1, \n",
    "                          validation_data=(LLD_test_FCN, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b73146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_FCN.history['acc'])\n",
    "plt.plot(hist_FCN.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.show()\n",
    "# Summarize history for loss \n",
    "plt.plot(hist_FCN.history['loss']) \n",
    "plt.plot(hist_FCN.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db389246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a63b23bb",
   "metadata": {},
   "source": [
    "# FRLM-去掉Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_FCN():\n",
    "    ip = Input(shape=(76, 500))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a9062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LSTM_FCN= LSTM_FCN()\n",
    "LSTM_FCN.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12991a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_LSTM_FCN = LSTM_FCN.fit(LLD_train_FCN, labels_train, batch_size=32, epochs=200, verbose=1,  \n",
    "                      validation_data=(LLD_test_FCN, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_LSTM_FCN.history['acc'])\n",
    "plt.plot(hist_LSTM_FCN.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.show()\n",
    "# Summarize history for loss \n",
    "plt.plot(hist_LSTM_FCN.history['loss']) \n",
    "plt.plot(hist_LSTM_FCN.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686df859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LSTM_FCN_loss = hist_LSTM_FCN.history['val_loss']\n",
    "df = pd.DataFrame(LSTM_FCN_loss , columns=['training loss'])#列名\n",
    "df.to_csv(\"C:/Users/10971/Desktop/Attention_LSTM/hist_LSTM_FCN_loss.csv\",index=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786d82e",
   "metadata": {},
   "source": [
    "# 去掉SE-block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALSTM_FC():\n",
    "    ip = Input(shape=(76, 500))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    #x = AttentionLSTM(8)(x)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c821194",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_FC= MALSTM_FC()\n",
    "LSTM_FC.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d9d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_LSTM_FC = LSTM_FC.fit(LLD_train_FCN, labels_train, batch_size=32, epochs=200, verbose=1,  \n",
    "                      validation_data=(LLD_test_FCN, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7612db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_LSTM_FC.history['acc'])\n",
    "plt.plot(hist_LSTM_FC.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.show()\n",
    "# Summarize history for loss \n",
    "plt.plot(hist_LSTM_FC.history['loss']) \n",
    "plt.plot(hist_LSTM_FC.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LSTM_FC_loss = hist_LSTM_FC.history['val_loss']\n",
    "df = pd.DataFrame(LSTM_FC_loss , columns=['training accuracy'])#列名\n",
    "df.to_csv(\"C:/Users/10971/Desktop/Attention_LSTM/hist_LSTM_FC_loss.csv\",index=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd622f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a9a33b",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_HSF():\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=-1, input_shape=(1, 1582)))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dropout(0.5))\n",
    "    #self.model.add(TimeDistributed(Dense(64)))\n",
    "    #self.model.add(Dense(64, activation='relu'))\n",
    "    #self.model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #LSTM_1.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HSF = LSTM_HSF()\n",
    "LSTM_HSF.compile(loss = 'mean_squared_error', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4494b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_LSTM = LSTM_HSF.fit(HSF_train, labels_train, batch_size=32, epochs=200, verbose=1,  \n",
    "                      validation_data=(HSF_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224751c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_LSTM.history['acc'])\n",
    "plt.plot(hist_LSTM.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.show()\n",
    "# Summarize history for loss \n",
    "plt.plot(hist_LSTM.history['loss']) \n",
    "plt.plot(hist_LSTM.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333ddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2a6930e",
   "metadata": {},
   "source": [
    "# 融合代码 1029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_forHSF():\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=-1, input_shape=(1, 1582)))\n",
    "    model.add(LSTM(1024))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #LSTM_1.add(Dropout(0.5))\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef205ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MALSTM_FCN_forLLD():\n",
    "    ip = Input(shape=(76, 500))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    #x = AttentionLSTM(8)(x)\n",
    "    x = AttentionLSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    # out = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, x)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e351e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLD_MALSTM_FCN = MALSTM_FCN_forLLD()\n",
    "HSF_LSTM = LSTM_forHSF()\n",
    "\n",
    "model_LLD = Sequential()\n",
    "model_HSF = Sequential()\n",
    "model_LLD.add(LLD_MALSTM_FCN)\n",
    "model_HSF.add(HSF_LSTM)\n",
    "\n",
    "model_combined = Sequential()\n",
    "model_combined.add(Merge([model_LLD, model_HSF], mode='concat'))\n",
    "model_combined.add(Dense(128))\n",
    "model_combined.add(Dropout(0.8))\n",
    "\n",
    "model_combined.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model_combined.add(Dense(4))\n",
    "model_combined.add(Activation('softmax'))\n",
    "# model_combined.add(Dense(64))\n",
    "# model_combined.add(Activation('relu'))\n",
    "# model_combined.add(Dense(4))\n",
    "# model_combined.add(Activation('softmax'))\n",
    "\n",
    "model_combined.summary()\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#model_combined.compile(loss='categorical_crossentropy',optimizer=adam ,metrics=['accuracy'])\n",
    "model_combined.compile(loss = 'mean_squared_error', optimizer = adam, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fusion_1029 = model_combined.fit([LLD_train_FCN,HSF_train], labels_train,  batch_size=128, epochs=200, verbose=1, \n",
    "                          validation_data=([LLD_test_FCN,HSF_test], labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_fusion_1029.history['acc'])\n",
    "plt.plot(hist_fusion_1029.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.show()\n",
    "# Summarize history for loss \n",
    "plt.plot(hist_fusion_1029.history['loss']) \n",
    "plt.plot(hist_fusion_1029.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi_Loss = hist_fusion_1029.history['val_loss']\n",
    "df = pd.DataFrame(Multi_Loss , columns=['training accuracy'])#列名\n",
    "df.to_csv(\"C:/Users/10971/Desktop/Attention_LSTM/Multi_loss.csv\",index=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1 = np.argmax(labels_test, axis=1)\n",
    "# 混淆矩阵\n",
    "y_pred = model_combined.predict([LLD_test_FCN,HSF_test])\n",
    "y_pred_1 = np.argmax(y_pred, axis=1)\n",
    "print('confusion_matrix:\\n',confusion_matrix(y_test_1, y_pred_1))\n",
    "print('classification_report:\\n',classification_report(y_test_1, y_pred_1,digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19583a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.signal\n",
    "\n",
    "path = 'hist_MALSTM_FCN.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "ABLSTM_FCN_acc = data['training accuracy']\n",
    "\n",
    "\n",
    "#ABLSTM_FCN_acc = scipy.signal.savgol_filter(ABLSTM_FCN_acc,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7699d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hist_LSTM_FCN.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "BLSTM_FCN_acc = data['training accuracy']\n",
    "#BLSTM_FCN_acc = scipy.signal.savgol_filter(BLSTM_FCN_acc,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hist_LSTM_FC.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "LSTM_FC_acc = data['training accuracy']\n",
    "#LSTM_FC_acc = scipy.signal.savgol_filter(LSTM_FC_acc,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a984e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'hist_LSTM_HSF.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "LSTM_HSF_acc = data['training accuracy']\n",
    "#LSTM_HSF_acc = scipy.signal.savgol_filter(LSTM_HSF_acc,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14747a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Multi_acc.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "multi_acc = data['training accuracy']\n",
    "#multi_acc = scipy.signal.savgol_filter(multi_acc,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f121ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LSTM_FC_acc)\n",
    "plt.plot(BLSTM_FCN_acc)\n",
    "plt.plot(LSTM_HSF_acc)\n",
    "plt.plot(ABLSTM_FCN_acc)\n",
    "plt.plot(multi_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Att-LSTM','MSBLSTM', 'URLM','FRLM','AMSBLSTM']) \n",
    "plt.savefig('ablation.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdab27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
